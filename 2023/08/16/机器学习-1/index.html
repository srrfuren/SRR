<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.srrnoseikai.xyz","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="知识点整理目录▶️ 第一章 绪论▶️ 第二章 模型的评估与选择▶️ 第三章 线性模型▶️ 第四章 决策树▶️ 第五章 神经网络▶️ 第六章 支持向量机▶️ 第七章 贝叶斯分类器▶️ 第八章 集成学习▶️ 第九章 聚类▶️ 第十章 降维与度量学习 第一章 绪论  考点概念：有无监督:有监督学习：有标准答案来判断模型的预测是否正确，有标记(分类和回归)无监督学习：没有标准答案告诉模型是否正确（聚类）半">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习">
<meta property="og:url" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="知识点整理目录▶️ 第一章 绪论▶️ 第二章 模型的评估与选择▶️ 第三章 线性模型▶️ 第四章 决策树▶️ 第五章 神经网络▶️ 第六章 支持向量机▶️ 第七章 贝叶斯分类器▶️ 第八章 集成学习▶️ 第九章 聚类▶️ 第十章 降维与度量学习 第一章 绪论  考点概念：有无监督:有监督学习：有标准答案来判断模型的预测是否正确，有标记(分类和回归)无监督学习：没有标准答案告诉模型是否正确（聚类）半">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/confuse_matrix.png">
<meta property="og:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/%E7%BA%A0%E9%94%99%E8%BE%93%E5%87%BA%E7%A0%81.png">
<meta property="og:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/mp%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B.png">
<meta property="og:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF%E7%9A%84%E8%AE%A1%E7%AE%97.png">
<meta property="og:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%A4%BA%E6%84%8F%E5%9B%BE.png">
<meta property="og:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F.png">
<meta property="og:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/%E8%BD%AF%E9%97%B4%E9%9A%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.png">
<meta property="og:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%AD%A3%E5%88%99%E5%8C%96.png">
<meta property="og:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E8%AE%BA.png">
<meta property="og:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B.PNG">
<meta property="og:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/%E7%94%9F%E6%88%90%E5%BC%8FVS%E5%88%A4%E5%88%AB%E5%BC%8F.png">
<meta property="og:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86.png">
<meta property="og:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.PNG">
<meta property="article:published_time" content="2023-08-16T15:44:26.000Z">
<meta property="article:modified_time" content="2023-08-16T16:07:33.730Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/image/confuse_matrix.png">

<link rel="canonical" href="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>机器学习 | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://www.srrnoseikai.xyz/2023/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Author">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-08-16 23:44:26" itemprop="dateCreated datePublished" datetime="2023-08-16T23:44:26+08:00">2023-08-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-08-17 00:07:33" itemprop="dateModified" datetime="2023-08-17T00:07:33+08:00">2023-08-17</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="知识点整理"><a href="#知识点整理" class="headerlink" title="知识点整理"></a>知识点整理</h1><h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p>▶️ <a href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BB%AA%E8%AE%BA">第一章 绪论</a><br>▶️ <a href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E5%92%8C%E9%80%89%E6%8B%A9">第二章 模型的评估与选择</a><br>▶️ <a href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B">第三章 线性模型</a><br>▶️ <a href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%86%B3%E7%AD%96%E6%A0%91">第四章 决策树</a><br>▶️ <a href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">第五章 神经网络</a><br>▶️ <a href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA">第六章 支持向量机</a><br>▶️ <a href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8">第七章 贝叶斯分类器</a><br>▶️ <a href="#%E7%AC%AC%E5%85%AB%E7%AB%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0">第八章 集成学习</a><br>▶️ <a href="#%E7%AC%AC%E4%B9%9D%E7%AB%A0-%E8%81%9A%E7%B1%BB">第九章 聚类</a><br>▶️ <a href="#%E7%AC%AC%E5%8D%81%E7%AB%A0-%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0">第十章 降维与度量学习</a></p>
<h2 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h2><blockquote>
<ul>
<li>考点概念：有无监督:<br>有监督学习：有标准答案来判断模型的预测是否正确，有标记(分类和回归)<br>无监督学习：没有标准答案告诉模型是否正确（聚类）<br>半监督学习：有也可以没有标准答案;在分类任务的训练集中常常同时包含有标签数据和无标签数据。无标签数据的分布帮助我们定义了同类样本的边界，少量有标签样本又为类提供了标签信息。</li>
<li>回归问题：无论y是否有穷，拟合一个直线<br>分类问题：y（输出）是有穷的，（本学期主要学的）</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>要点：训练集和测试集不能有共同数据</li>
</ul>
</blockquote>
<ol>
<li>机器学习的本质：对经验的利用，对新情况做出有效的决策。而在计算机中，经验通常以数据存在。</li>
<li>基本术语：</li>
</ol>
<table>
<thead>
<tr>
<th>术语</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td>学习器</td>
<td>一个模型就是一个学习器</td>
</tr>
<tr>
<td>数据集</td>
<td>一整个数据表</td>
</tr>
<tr>
<td>一个示例（样本,instance）</td>
<td>一行数据（有所有属性的一个取值），一个对象，一个特征向量</td>
</tr>
<tr>
<td>属性（特征）</td>
<td>反映对象在某方面的表现或性质的事项</td>
</tr>
<tr>
<td>属性值（属性空间）</td>
<td>属性的取值（属性组成的空间）</td>
</tr>
<tr>
<td>标记（label）</td>
<td>示例的结果（有了结果的示例，合起来叫样例（example)</td>
</tr>
<tr>
<td>未见样本</td>
<td>模型没有训练和测试过的样本</td>
</tr>
<tr>
<td>泛化能力</td>
<td>模型已经训练好之后，对未见样本的预测能力的大小，预测效果如何，如果结果要好则称其泛化能力强。（针对测试的过程）</td>
</tr>
</tbody></table>
<ol start="3">
<li>假设集合：比如分类西瓜，所有可能的属性取值可以组成一个空间，有些属性配对会和实际是好瓜相符，有些不会，把不相符的属性配对删除，剩下的符合的假设就是假设集合。真相只有一个，假设往往有很多个。</li>
<li>任何一个有效的机器学习算法必有其偏好，需要与问题本身匹配，若没有具体问题，一般模型选择原则——奥卡姆剃刀（越简单越好）</li>
<li>NFL（no free lunch）定理：若算法A比B在某些方面要好，B比A也在某些方面要更好。”天下没有免费的午餐！”脱离具体问题谈哪个算法好是没有意义的。</li>
</ol>
<h2 id="第二章-模型的评估和选择"><a href="#第二章-模型的评估和选择" class="headerlink" title="第二章 模型的评估和选择"></a>第二章 模型的评估和选择</h2><p><em><strong>#最重要的一章#</strong></em></p>
<blockquote>
<p>重点：误差，欠、过拟合，偏差与方差的结论<br>怎么划分数据集（三个方法里考一个）<br>怎么评估结果（正确率、错误率、查全率、查准率、ROC曲线）<br>比较不同模型的优劣（了解就可以了，假设检验）</p>
</blockquote>
<ol>
<li><p><strong>误差</strong><br>错误率：E&#x3D;a&#x2F;m;<br>精度(accuracy)&#x3D;1-a&#x2F;m;<br>误差(error)：学习器的实际预测输出与样本真实输出之间的差异;<br>训练误差(training error)&#x2F;经验误差(empirical error)：学习器在训练集上的误差;<br>测试误差：在测试集上的误差<br>泛化误差(generalization error)：在新样本上的误差。  </p>
</li>
<li><p><strong>过拟合</strong><br>  什么模型好？泛化能力强的<br>  泛化误差应该越小越好；<br>  经验误差不是越小越好，因为可能会出现过拟合；<br>  过拟合（overfitting）：训练程度太过了，模型的学习能力太强，使得不是特别关键的特征也认为是关键的特征<br>  欠拟合（underfitting）：训练程度不够，模型的学习能力低下，使得模型没能学习到关键的主要特征  </p>
</li>
<li><p><strong>模型的选择和评估</strong><br>划分测试集的<strong>原因</strong>：训练集普遍存在过拟合的问题，因为我们运行一个学习模型必是在多项式时间内完成的，即P,而现实中我们求解的问题往往是只能在多项式时间内验证而无法求解的，即NP，如果我们能够依靠多项式时间而找出最优解，彻底避免过拟合，这就意味着P&#x3D;NP，显然这目前是做不到的，所以训练集普遍存在过拟合，因此我们需要划分另一个集合来测试模型，保证不在训练集中出现和使用过，于是就有了测试集。<br><em><strong>常见方法</strong></em></p>
</li>
</ol>
<ul>
<li><p><strong>留出法</strong>（<strong>直接划分一个比例</strong>）：<em><strong>保持训练集的数据分布与数据集的的一致性</strong></em>（<strong>分层采样</strong>，从好瓜和坏瓜里都取30%拿去训练），多次重复划分最后模型评估求平均（如果只划分一次，那测试集的信息就是浪费掉了），不能太大也不能太小1&#x2F;5~1&#x2F;3（比例确定）；</p>
<blockquote>
<p>Tips：留出法的困境：只拿百分之30%左右的样本作为测试集，由于比较少，评估的结果可能<strong>不够准确</strong>，但是又不能减少训练集的样本数给测试集，因为这样的话用训练集训练出来的模型可能会和原数据集D训练出来的模型大相径庭，和我们原本的目的相违背，降低了评估结果的<strong>保真性</strong>。因训练样本规模不同而导致的估计偏差。</p>
</blockquote>
</li>
<li><p><strong>k-折交叉验证法（针对留出法的重复划分的内容——数据重用）</strong>：k是根据具体数据自己定的，即把数据分成k份；对这些数据进行k次建模。第一次拿k-1份当训练集，剩下的一个当测试集，不断换着做k次训练，出来k个模型，最后返回这k个模型的平均结果。（若k&#x3D;m,m是数据集数量，则每一个样本都单独作为一份数据，即留一法（LOO,leave-one-out），较少这么用）</p>
<blockquote>
<p>k最常的取值为10，模型评估结果的稳定性和保证性很大程度上取决于k的取值。10次10折交叉验证是指，重复做十次的10折交叉验证求平均。<br>缺陷：在数据集比较大的时候，会要训练很多个模型，计算复杂度高，如果还要调参的话，要花的时间更久了，所以：NFL，天下没有免费的午餐。</p>
</blockquote>
</li>
<li><p><strong>自助法（基于“自助采样”，有放回采样，可重复采样，bootsrap sampling）</strong>：可保证每一次采样，样本被采到的概率是一样的都是n分之一。样本一次都没有被抽到过的概率，lim（1-1&#x2F;n)^n&#x3D;1&#x2F;e,大约36.8%，即可将这些没有被抽到的样本作为测试集，其他的至少被抽到一次的样本作为训练集，这就是该方法划分的思路。“外包估计”（这样的方法，训练集和原样本集同规模，只不过数据分布有所改变，这个影响不大，但数据量足够的话还是用前面两个方法好）</p>
<blockquote>
<p>这种方法在<strong>数据集较小</strong>、难以有效划分训练和测试集时很有用，且能从原始数据集中产生多个不同的训练集，这对集成学习很有帮助。减少训练样本规模不同而导致的估计偏差，又比较高效的进行实验估计。</p>
</blockquote>
</li>
</ul>
<ol start="4">
<li><strong>调参与最终模型：</strong></li>
</ol>
<ul>
<li>算法的参数：一般有人工设定，亦称为超参数。  </li>
<li>模型的参数：一般由学习确定。   </li>
<li>验证集（在训练的过程中，把其作为测试集去使用）是为了设定超参数新设置的一个集合。好和不好，看泛化能力。产生原因：测试集是所有参数都确定下来才能碰的，所以要<strong>设定调节超参数只能从训练集集里划一部分出来作为验证集</strong>，当成临时的测试集使用，来建立模型。通过泛化能力确定最好的模型。在超参数定下来之后，才把验证集和训练集合并起来成为完整的训练集重新训练到最终模型，最后拿测试集来进行测试。 </li>
<li>测试集上的判别效果来估计模型实际使用时的泛化能力，而基于验证集上的性能来进行模型的选择和调参。   </li>
<li>调参过程相似：先产生若干模型，然后基于某种评估方法进行选择。</li>
</ul>
<ol start="5">
<li><strong>性能度量</strong> 对泛化性能进行评估,用哪些指标刻画模型的好或者不好？<br>回归任务：<em>均方误差</em> $E(f;D)&#x3D;\frac{\bm 1}{\bm m}\sum\left(f(x_i)-y_i\right)^2$<br>分类任务：<em>正确率</em> $E(f;D)&#x3D;\frac{\bm 1}{\bm m}\sum_{i&#x3D;1}^m \amalg(f(x_i)\ne y_i)$<br>$\amalg$ : 满足括号里的条件就返回1，不满足就返回的0。f指模型，x指样本，y指实际的分类结果。  <blockquote>
<p>更一般的有概率密度函数p(x)，那么公式可以这样写。<br><em>均方误差</em> $E(f;D)&#x3D;\int_{x\sim D}\left(f(x)-y\right)^2 p(x) dx.$<br><em>正确率</em> $E(f;D)&#x3D;\int_{x\sim D} \amalg(f(x)\ne y)p(x) dx.$</p>
</blockquote>
</li>
</ol>
<p>查准率（准确率）：P&#x3D;TP（真正例）&#x2F;TP+FP（假正例）<br>查全率（召回率）：R&#x3D;TP（真正例）&#x2F;TP+FN（假反例）  </p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">预测结果</th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>真实情况</strong></td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">TP（真正例）</td>
<td align="center">FN（假反例）</td>
</tr>
<tr>
<td align="center">0</td>
<td align="center">FP（假正例）</td>
<td align="center">TN（真反例）</td>
</tr>
</tbody></table>
<blockquote>
<p>但是注意在python中混淆矩阵的输出是这样的：<br><img src="image\confuse_matrix.png" /><br>且查准率和查全率是一对矛盾的度量，一般一个高另一个就会低。</p>
</blockquote>
<ol start="6">
<li><strong>PR图（一般查全率是横坐标，查准率是纵坐标）：</strong><br>一条PR曲线对应着一个学习器，怎么画的曲线：当一个模型被确定下来时，只有一组P值和R值（就是只有一个点），那么是怎么画出来的曲线呢？其实调整曲线的是判别的标准，即阈值（若取0.5，则表示当概率大于0.5时归到0这一类，若&lt;&#x3D;0.5是归到1这一类，这个概率越小，说明要尽可能归到0这一类（查全率），宁杀错不放过，这个概率越大，说明更可能归到1类（查准率），宁放过不杀错）。这样，<strong>不同阈值</strong>下就有不同的点，可以连接或者平滑化成一条曲线。</li>
</ol>
<ul>
<li>怎么看曲线：<br>一、如果一条曲线被另一条曲线包在里面，那么外面曲线对应的学习器比在里面曲线对应的学习器更好，因为无论如何比较两条曲线，外面那条曲线的查准率和召回率都比里面的那条要好。<br>二、如果一部分在外面，一部分在里面，则用到BEP的指标(平衡点指标），即从图里画一条45°的直线，在这条直线上的点查全率和查准率是相等的，其与曲线相交我们称为平衡点，看看哪条曲线的平衡点在最外面，说明平衡点查全率和查准率的值更大，那么说明这个学习器比里面的学习器要更好。<br><code>F1度量指标</code>:了解一下就好，F1度量是基于查全率和查准率的调和平均，度量准则是F1值越大，查准和查全率越大，算法性能越好；Fβ则是查全率和查准率的加权调和平均（帮助更好的倾向于查全还是查准）</li>
</ul>
<ol start="7">
<li><p><strong>宏、微：</strong><br>多次的重复试验（得到多个混淆矩阵），计算宏指标和微指标，来计算总的查准率和查全率、F1度量。<br>宏查准率：所有的查准率加起来除以n<br>微查准率：所有的真正例加起来除以n求均值&#x2F;所有的真正例加起来除以n求均值+所有的假正例加起来除以n求均值</p>
</li>
<li><p><strong>ROC和AUC</strong><br>ROC曲线（受试者工作特征曲线，横坐标假正例率，横坐标真正例率）：<br>假正例率：FP&#x2F;FP+TN（真反例）<code>反例里的假真例</code><br>真正例率（查全率）：TP&#x2F;TP+FN<code>正例里的真正例</code><br>当取阈值为1即最大时，所有的样本都会被判断为反例，这时候也就没有真正例TP和假正例FP了，只有真反例TN和假反例FN,所以计算出来的ROC坐标点为（0，0）；当阈值取为0即最小时，所有的样本会被判断为正例，这时候就没有假反例FN和真反例TN，只有假正例FP和真正例TP，所以计算出来的ROC坐标点为（1，1）.<br>ROC曲线下的面积大小(AUC)可以衡量好坏，越大越好。主要看前面部分，当横坐标严格时，纵坐标够不够大，就是有没有一下子升的很高的意思。<br>$AUC&#x3D;\frac{1}{2}\sum_{i&#x3D;1}^{m-1}(x_{i+1}-x_i)\cdot(y_{i+1}+y_i)$</p>
</li>
<li><p><strong>非均等代价</strong>：<br><strong>在混淆矩阵里对应FN,FP是犯不同错误造成的结果，实际问题里的着重点不一样</strong>。在原来混淆矩阵的基础上，设一个代价矩阵，代价敏感错误率：分别统计不同类错误的群体，给他一个权重计算，来进行描述非均等代价的问题。<br>代价矩阵:  </p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">预测类别</th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="center">真实类别</td>
<td align="center">第0类</td>
<td align="center">第1类</td>
</tr>
<tr>
<td align="center">第0类</td>
<td align="center">0</td>
<td align="center">cost_01</td>
</tr>
<tr>
<td align="center">第1类</td>
<td align="center">cost_10</td>
<td align="center">0</td>
</tr>
</tbody></table>
</li>
<li><p><strong>如何判断实质性差别</strong>：<br>一、单单从测试集指标正确率错误率的数值，<strong>测试性能不等于泛化性能</strong>，<strong>测试性能随着测试集的变化而变化</strong>，<strong>机器学习算法本身存在随机性</strong>，所以去判别两个模型的好坏是不一定的<br>二、（要比较的话，必须在统计学的意义下去保证我们的模型对另外一个模型的性能有显著的提升）假设检验：两学习器比较：t检验、F检验、McNemar检验；多学习器：Friedman、Nemenyi后续检验 书本2.4.2节，其实就是假设变了，检验统计量变了（这部分只做了解就好）。通过假设检验判断是否有显著性差异，再通过相关指标大小做出优劣判断。</p>
</li>
<li><p><strong>偏差-方差分解（bias-variance decomposition）</strong><br>泛化误差来表示模型对应的泛化能力，下面偏差-方差的分解就是在对学习算法的期望泛化进行拆解。<br>$E(f;D)&#x3D;E_D[(f(x;D)-y_D)^2]$</p>
</li>
</ol>
<p>#这里就是指通过模型输出的预测值和数据集标记计算期望（平均）误差。<br>$&#x3D;E_D[(f(x;D)-\bar f(x)+\bar f(x)-y+y-y_D)^2]$ ,已知噪声的期望为0，化简后可得如下<br>$E(f;D)&#x3D;bias^2(x)+var(x)+\varepsilon^2$<br>见课本推导式子P45，掌握结论不求证明。泛化误差可分解为<strong>偏差、方差与噪声之和</strong>。<br>其中$bias^2$：偏差，期望输出与真实标记的差别<br>$var（x)$：方差，同样大小的不同训练集的变动，所导致的性能变化,产生模型输出与期望输出的方差<br>$\varepsilon^2$：噪声，表达了当前任务上任何学习算法所能达到的期望泛化误差下界（<strong>训练样本标记与真实标记有区别</strong>），采集过程无法避免的误差。无法控制。所以主要关注前面两个。<br>体现出泛化性能是由<strong>学习算法的能力（偏差）</strong>、<strong>数据的充分性（方差）</strong>、<strong>学习任务本身的难度（噪声）</strong>，三方面共同决定的。</p>
<ul>
<li>一般而言，偏差与方差存在冲突：（泛化误差与偏差、方差的关系示意图）  <ul>
<li>训练不足时，学习器拟合能力不强（欠拟合），训练数据的扰动不足以使得学习器产生显著的变化。偏差主导。（泛化误差大）  </li>
<li>随着训练程度加深，学习器拟合能力逐渐增强，训练数据发生的扰动能渐渐被模型学习到，方差逐渐主导。(泛化误差变小）  </li>
<li>训练充足后，学习器的拟合能力很强（过拟合），训练数据发生轻微的扰动都会导致学习器发生显著的变化，方差主导。（泛化误差又增大）</li>
</ul>
</li>
</ul>
<h2 id="第三章-线性模型"><a href="#第三章-线性模型" class="headerlink" title="第三章 线性模型"></a>第三章 线性模型</h2><p>线性有两种类型：分类和回归； </p>
<blockquote>
<p>线性判别分析的思想，不需要推导过程<br>多类型学习的三种方法：重点ECOC<br>怎么判断类别不平衡（特征）？有什么后果？怎么解决？ </p>
</blockquote>
<ol>
<li><p><strong>线性判别分析（LDA）：</strong><br>用降维的方法来进行分类，将高维数据投影到低维数据。要使得投影的点，同类之间要尽可能的接近，不同类的点要尽可能的远离。若能达到这个效果，就是一个比较好的分类模型。<br><strong>思想</strong>：给定训练样例集，设法将样例投影到一条直线上，使得同类的投影点之间要尽可能的接近，不同类的投影点要尽可能的远离，在对新样本分类时也是投影到这条直线上，根据位置做出类别判断。求出直线w向量，量化目标。<br>由于该方法可通过投影来减小样本点的维数，且投影过程中使用了类别信息，所以LDA常被视为一种经典的监督降维技术。</p>
</li>
<li><p><strong>多分类学习（有两个以上的结果）：</strong><br>一般可用逻辑回归、神经网络，但有一些模型只能进行二分类学习、当我们又想用在多分类上（决策树）。<br>拆解法：把一个多分类任务拆分为若干个二分类的任务求解。<br>一、<strong>one vs one（OVO）思路：</strong><br>既然要分成若干任务，直接在多分类里挑两个出来做二分类就好了。把所有可能性的组合都取一遍，一个做正类，另外一种做反类。那么如何综合结果得出最终的分类结果呢？用<strong>投票法</strong>（少数服从多数），一共有Cn2种排列结果。<strong>缺点</strong>：可能要训练的模型会很多。<strong>优点</strong>：训练时间会更短<br>二、<strong>one vs rest（OvR）思路：</strong><br>从多分类中选一个做正类，剩下的属于反类，然后预测正反来看。用投票法得出最后的结果，如果是正类就加一票，如果是反类就所有反类里的类别都加上一票。<strong>优点和缺点跟OVO反过来</strong>。不过多数情况下预测性能两者差不多。（期末考试）<br>三、<strong>many vs many(MVM)思路：</strong><br>若干类做正类，若干类做反类。划分次数自己定。MVM的正、反构造必须有特殊的设计，不能随意选取，于是有了“纠错输出码”。编码和解码。打横看哪几类划分为正类，哪几类划分为反类编为1和-1，0表示不使用该样本，这就是编码，测试样本输进去，输出结果，看测试结构的编码和上面那个类的编码最接近就归到那个类里面，这个过程就是解码。<br><strong>比较程度</strong>：看海明距离和欧氏距离。<strong>海明距离</strong>：看不一样的编码个数。要找最接近的那个，不一样的个数越少越接近，就是距离最短的这个。<strong>欧氏距离</strong>：把每一个类别的编码直接看成数值向量，向量间相减平方和开根号。<br><img src="image\纠错输出码.png"><strong>(考试！！！！！！！)</strong></p>
</li>
</ol>
<p><em>为什么要称为纠错输出码呢？</em> 因为ECOC编码对分类器的错误有一定的容忍和修正能力，对同一个学习任务来说，一般<strong>ECOC编码越长</strong>，纠错能力越强，对于同样长度的编码，任意<strong>两个类别之间的编码距离越远的话</strong>，其纠错能力越强。但需要训练的分类器也会越多，计算开销会大，且码太长了对于有限类别组合来说可能失去意义。而且也不一定说类别之间的编码距离越远越好，NP难问题没有最优编码。</p>
<ol start="4">
<li><strong>类别不平衡：</strong><br>定义：分类任务中不同类别的训练样本例数差别很大；后果:很有可能模型会偏向数量比较多的那一类；例如950个好瓜 50个坏瓜，即使全部判断为好瓜，也有95%的正确率。<br>基本应对策略：“再缩放”，调整阈值。如逻辑回归是以0.5作为阈值，大于0.5的一类，小于0.5的一类。而若把阈值调为0.95，则大于0.95才为一类，即对数量比较多的一类更加严格。然而精准估计m+&#x2F;m-是很难的。<br>常见类别不平衡的学习方法：<strong>过采样</strong>重复采样、(SMOTE）使用插值；*<em>欠采样</em>，例如从950个好瓜里抽50个出来和50个坏瓜训练模型，即去除一些样本使得正反比例一致。（EasyEnsemble）：利用集成学习机制，将反例划分为若干个集合供不同学习器使用；阈值移动就是上面讲的再缩放。</li>
</ol>
<h2 id="第四章-决策树"><a href="#第四章-决策树" class="headerlink" title="第四章 决策树"></a>第四章 决策树</h2><pre><code>怎么构建的？
最优判别属性的三个选择（指标），怎么使用？（那个选最大，哪个选最小？）
怎么防止过拟合？
缺失值处理
</code></pre>
<ul>
<li><p>决策树（监督学习）：<br><strong>构建过程</strong>：一般的，一颗决策树包含一个根节点，若干个内部节点和若干个叶结点。<br>内部结点：对应某个<strong>属性</strong>的“测试”<br>分支：对每个测试结果可能的取值<br>叶结点：对应一个”预测结果”<br>每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根节点包含样本全集，从根节点到每个叶结点的路径对应了一个判定测试序列。</p>
</li>
<li><p>学习（训练）过程：通过对训练样本的分析来确定“划分属性”<br>预测（测试）过程：将测试事例从根节点开始，沿着划分属性所构成的“判定测试序列”下行，直到叶节点。（输入新样本看输出）<br>越重要的、首先考虑的属性放在根节点或离根节点越近的地方。</p>
</li>
<li><p>目的：产生一颗泛化能力强的树<br>思想：“分而治之”，自根至叶的递归过程<br><strong>递归停止条件</strong>：一、当前节点包含的样本全属于同一类别，无需划分；二、当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；三、当前结点包含的样本集合为空，不能划分。</p>
</li>
</ul>
<blockquote>
<p>算法实现：<br>输入：训练集{xi，yi}x是样本点，y是结果和属性集即所有属性的集合。<br>输出：以node为根节点的一颗决策树。<br>过程：函数TreeGenerate(D（训练集）,A（属性集),函数中调用函数实现递归。<br>1.生成结点node<br>2.判断生成的节点是作为内部节点还是叶节点（即递归停止条件一）<br>3.如果A（属性集）是空集或者这些样本在属性上的取值是否一样（即递归停止条件二），则标记该结点为叶结点，然后取样本里数量最多的类别作为其类别<br>4.从属性集中选择最优的划分属性a*（<strong>决策树算法的核心</strong>），<strong>即根结点放哪个属性最好</strong>。可以用各个指标来进行判断哪个属性最好：信息增益、增益率、基尼指数。<br>5.对最优划分属性a*中每一个取值都进行一次循环：为这个结点node生成一个分支，把对应属性中的取值一样类别的样本给挑出来放到样本子集Dv里，不一样的放到另一个子集里去。<br>6.判断样本子集Dv是不是空集，如果不是空集则调用TreeGeneratr函数（Dv，A{a*}）把当前的最优属性集去掉。（离散一般去除，而连续可以不去重复使用），如果是空集只能标明该结点为叶结点，叶结点的输出结果没有判定依据，只能以概率最大的一类作为其输出结果。（即递归停止条件三）</p>
</blockquote>
<ul>
<li><p>最优的划分属性的指标：<br><strong>信息增益</strong>：用信息熵构造的，（信息熵：是度量样本集合“纯度”最常用的一种指标），目标是为了放进的样本增加的信息量要最高，生成内部结点之前和之后的信息熵的差来计算。把每个属性都放进去算信息增益看看哪个<strong>最大</strong>。划分后的信息熵：用不同的分支给不同的权重（样本越多越重要），加权来计算划分后的信息熵。<strong>缺点</strong>：对可取值数目较多的属性有所偏好。若把编号也作为一种属性，那信息增益会很大，但其实对泛化一点帮助都没有。<br>怎么计算：首先计算根结点信息熵：$正例比例<em>log_2正例比例+反例比例</em>log_2反例比例$；然后计算按属性分开之后的各属性值的信息熵，最后用根结点的信息熵减去（各属性值占该属性样本数的比例* 各属性值的信息熵，求和） </p>
</li>
<li><p><strong>增益率</strong>：对可取值数目较少的属性有偏好。属性A的可能取值数目越多（V越大），增益率越小。<strong>把信息增益超出平均值的对应的属性</strong>，先挑出来作为候选属性，至于超出的里面那个是最好的，可能受到偏好的影响，这时候<strong>再使用增益率最高</strong>来挑选。  </p>
</li>
<li><p><strong>基尼指数</strong>：对应一种概率来表示属性优劣，随机抽两个样本<strong>不一样的概率</strong>是多少。不一致的概率越小，数据集的纯度越高。<strong>基尼指数选最小</strong>的那个</p>
</li>
</ul>
<blockquote>
<p>如果最后发现模型的应用效果不是很好的话，该怎么办？<br><strong>剪枝</strong><br>研究表明：指标的选择对泛化能力性能的影响是很有限的，所以我们需要找另外的方法。<br>因为决策树学习的是结构，所以优化调参其实就是调整其结构。<br>剪枝：主动去掉一些分支来降低<strong>过拟合</strong>的风险，有时候模型学习训练样本学得太好了，以至于把训练集自身的一些特点当作所有数据都具有的一般性质而过拟合。一、<strong>预剪枝</strong>，边生成边判断哪些分支要保留。二、<strong>后剪枝</strong>，生成一个完整的决策树后，才决定是否剪枝。</p>
</blockquote>
<p><strong>预剪枝</strong>：首先算一下不划分的验证集精度：测试集正确数&#x2F;测试集总数（<em>测试集的正确数是顺着整棵树看下来的</em>），再算一下按最优划分指标决定的属性划分后的验证集精度，如果划分前的验证集精度大于等于划分后的验证集，就禁止划分了。每个结点都尝试一遍，直到不能再划分。时间开销低，但有可能增加欠拟合的风险。因为每走一步都有可能剪枝，它是一个局部的最优，而不一定是整体的最优。<br><strong>后剪枝</strong>：剪属性结点，从一个完整的决策树自底向上开始剪分支，如果前后一样可以不剪枝了（一般要剪的）。时间开销高，但欠拟合风险基本不变。泛化性能通常后剪枝更好。</p>
<ul>
<li><p><strong>连续值</strong>：基本思路是连续属性离散化。<br><strong>常见做法</strong>：<strong>二分法</strong>。我们需要找到划分的那个点会使结果达到最优。例如；17个样本点可以有十六个区间，那么在这十六个区间里面取点，无论是区间的哪个地方取，取的结果都是一样的，所以我们就决定取中间的点作为候选点。每个划分点都去算一次信息增益，看看十六个信息增益哪个<strong>最大</strong>，就按其去划分。<em>注意：连续属性作为划分结点后，该属性还可以作为其后代结点的划分属性。</em></p>
</li>
<li><p><strong>缺失值</strong>：指某一些样本在某一些属性里是没有属性值的。这在训练的过程就很难办，不知道把缺失值的样本归为哪一类，而且如果各属性缺失值是不一样的属性的话，这样计算就不合理。直接去掉缺失值样本又对数据造成极大的浪费。<br>需要解决：如何在属性值缺失的情况下进行划分属性选择？若给定划分属性，样本在该属性上的值缺失，如何对样本进行划分？<br><strong>基本思路</strong>：样本赋权，权重划分。先用已有的数据去计算指标，再设置权重去决定最后的结果。不一样的地方在于信息熵的计算公式，其中的<strong>权重是无缺失值样例中属性a取值为v的占无缺失值样例的比</strong>。算出来之后，<strong>再乘无缺失值样例的占总样例的比</strong>，这就解决了划分属性选择的问题。再到如何对样本划分：若样本在这个属性上缺失则同时进入该属性的所有分支，并按照不同属性取值的比例设置它在不同属性里的权重，其他无缺失值的样本权重均为1。</p>
</li>
</ul>
<blockquote>
<p>一棵决策树对应一个规则集，从根节点到叶结点的分支路线对应一个规则。可改善可理解性  </p>
</blockquote>
<ul>
<li>单变量决策树：<br>轴平行（因为划分的线和轴是<strong>平行</strong>的）划分，用分类的平面或超平面也能分割出结果（<strong>连续型数据的才能直观的画出来</strong>），只要左右两边或者上下两边都是好瓜或者坏瓜就能去掉这里的线段，那剩下的线就构成了一个分类平面。  </li>
<li>多变量决策树：每个非叶结点不仅考虑一个属性。画<strong>线性函数</strong>分类，非轴平行，不再是为非叶结点找一个最优划分属性而是一个合适的线性分类器。  <blockquote>
<p>分类边界复杂是时，需要非常多段划分才能获得较好的近似。产生了类似曲线的边界。</p>
</blockquote>
</li>
</ul>
<h2 id="第五章-神经网络"><a href="#第五章-神经网络" class="headerlink" title="第五章 神经网络"></a>第五章 神经网络</h2><pre><code>重点是计算前馈过程
反馈过程的BP算法不要求推导，理解思想就好
感知机的作用和存在的问题
如何防止落入极小值点？
其他常见的神经网络只做了解
</code></pre>
<ul>
<li><p>深度学习的基础<br>什么是神经网络？来源于生物学的名词<br>M-P神经元模型，xi是输入(某个样本在这些属性上的取值），或者上一层神经元的输出，线是连接，wi是连接权重（哪些属性重要值就大）。</p>
<img src="image\mp神经元模型.png">
神经网络学得的知识蕴含在连接权与阈值中。（wi*xi加起来，就是整合的过程，例如果整合的结果大于等于Θ，就输出正类，否则就输出反类），一般都把Θ移到左边输入，让机器也自己学习，然后和阈值选为**0**进行比较。然后通过激活函数处理以产生神经元的输出。有阶跃函数：x>=0,函数值取1，<\0，函数值取0；有sigmoid函数，函数值$=\frac{1}{1+e^{-x}}$
>输入-->整合-->激活-->输出
</li>
<li><p><strong>多层前馈网络结构</strong>：第一层叫输入层；最底一层叫输出层；中间层叫隐（视）层。全连接：两层之间的神经元全部链接起来。前馈网络：流程是能一步一步往前传，从输入到输出。隐层和输出层才是真正是神经元的层，所以称为“功能单元”，不像输入的层仅仅是数据的输入，而不是神经元的抽象。</p>
</li>
</ul>
<blockquote>
<p>前馈神经网络的计算（这里面的激活函数是阶跃函数，且该多层前馈神经网路解决的是异或问题）：<br><img src="image\前馈神经网路的计算.png"><br>绿色：$&#x3D;sgn(1<em>1+1</em>1-0.5)&#x3D;1$<br>蓝色：$&#x3D;sgn(1*(-1)+1*(-1)-(-1.5))&#x3D;0$<br>粉色：$&#x3D;sgn(1<em>1+0</em>1-1.5)&#x3D;0$</p>
</blockquote>
<ul>
<li><p>二分类的话，就用一个或者两个神经元就能输出了，（输出层的神经元个数是可以确定下来的），其次主要是确定多少层隐视层（看激活函数是什么），然后确定隐视层中神经元的个数（没有具体的方法，还没有解决的问题）。</p>
</li>
<li><p>神经网络的缺点：功能虽然强大，但是训练好的模型让人不知道神经网络到底做了什么？（黑箱模型），所以有些大公司都不太喜欢用这个模型，他们追求可解析性。</p>
</li>
<li><p>神经网络只要结构足够复杂，就能以任意精度逼近任意复杂度的连续函数。哇。分类能力很强。</p>
</li>
<li><p><strong>感知机</strong>（两层神经网路：输入层、输出层）（可解决线性可分问题，不可解决线性不可分问题）<br>与：两个都为真返回，真否则返回假；<br>或：其中一个为真返回真……；<br>非: 若输出为真，则输出就要为假，反之亦然……；<br>异或：一个真一个假为一类，两个真两个假为另一类。<br>（与问题，或问题，非问题是线性可分问题）<br>（异或是线性不可分问题）<br>为解决异或就多加神经网络层数。  </p>
</li>
<li><p>神经网络的灵魂主要在学习的过程：（BP算法，误差逆传播算法）<br>例如选权重，一开始初始的权重是随机选取的，对应的误差项相对比较大，<strong>BP学习的思想</strong>：目的是均方误差最小，即使目标模型预测结果和实际的结果越接近越好，能不能有一个方法经过学习过程之后能不断逼近极小值点。利用对应的点的梯度，即切线的方向，再给定某一个步长，让对应的wi往这个方向移动某个步长的大小，不断得到新的权重，则其会不断逼近最小值。（<strong>梯度下降法</strong>）。<strong>学习率</strong>是超参数（控制其逼近极小值点的速度），学习率高步长设置的就大;超参数调参，看看哪个模型率会更好一些。学习率<strong>不能太大</strong>，不然会出现在极小值点来回震荡的现象，永远逼近不了最小值点。<strong>太小的话</strong>，学习就会非常非常慢。</p>
</li>
<li><p>每个样本都从输入到输出更新一次对应的新参数，则称经过了一轮的训练。<br>停止条件，前后两轮误差项在e之内；或者是训练多少多少轮停止。<br><strong>批量梯度下降</strong>：取十个样本，先令一些样本有输出，再批量的更新，这样输出快一点，效果好但训练耗时慢。更新每一参数时都使用所有的样本来进行更新。即等所有样本训练学习后再更新每一参数。<br>优点：全局最优解；易于并行实现；<br>缺点：当样本数目很多时，训练过程会很慢。<br><strong>随机梯度下降：</strong> 随机抽十个，在更新每一参数时都使用一个样本来进行更新，每一次更新参数都用一个样本，更新很多次。即每训练学习一个样本就更新一次权重。<br>优点：训练速度快；<br>缺点：准确度下降，并不是全局最优；不易于并行实现。</p>
</li>
</ul>
<blockquote>
<p>当样本训练集非常大时，标准BP往往比累积BP更快获得较好的解。标准BP是一个样本更新一次，累积BP是累积误差最小化，整个训练集训练一遍后才更新。</p>
</blockquote>
<ul>
<li><p><strong>缓解过拟合</strong>：训练到一定程度已经没办法改善误差，再训练下去会增大方差。有些权重越来越大，有些越来越小，越来越大的没有约束。<br>早停：训练轮次约束，或者使用验证集，但训练误差下降而验证误差上升时停止<br>加上了正则化项：（1-λ）wi^2求和，也要最小；使得权重变小的偏好更大一些。<br>Drop out（结构复杂的时候使用）：随机挑出一些权重把其设为0，防止其过拟合。</p>
</li>
<li><p>容易落入极小值点，而且就很难出来了，因为点的<strong>更新</strong>就是根据梯度，而极小值点梯度等于0；（很难去避免，所以对于这个问题需要识别什么时候是<strong>局部极小</strong>）</p>
</li>
<li><p>判断是否落入极小值点？怎么跳出来？（像黑洞一样，好有意思哈哈）<br>如果<strong>模型一直保持在一个比较低的水平</strong>（50%~70%)，则是说明其落入到局部极小的标志。<br><strong>初始参数的位置</strong>的选取也很重要，所以可以通过使用不同的初始参数，这样可能落入不同的极小值点，从中选择可能最接近全局最小的，以此跳出极小值点；<br><strong>通过一些计算机算法</strong>（模拟退火：在每一步有一定概率接受比当前解更差的结果、遗传算法）<br><strong>随机扰动</strong>：随机扰动的大小。即随机梯度下降法，在计算梯度加入了随机因素，即使陷入了局部最小值点，梯度也不会为0。<em>随机扰动的数值大小决定了算法每次搜索的步长，若扰动太大，容易步子迈大扯着蛋，难以搜索到全局最优解</em>。<br>神经网络找到一个相对比较好的局部极小，毕竟很难去得到一个全局最小，且最后从结果看也不知道是不是全局最小，<strong>我们找到一个比较好的极小点也可以，我们是避免落入比较差的极小点。</strong></p>
</li>
</ul>
<blockquote>
<p>神经网络延伸：深度神经网络，一个是增加隐层的层数、一个是增加隐层的神经元。<br>往深度发展会更好，浅层的发展对正确率的改变并不大。</p>
<p>正确率急速下降的原因：梯度消失，不能仅仅增加隐层的数量。<br>为什么会出现梯度消失？主要就是由于激活函数的导数相乘的结果，层数越多，激活函数的导数乘起来会使得偏导数非常非常小，梯度就越小，那么权重的更新就更新不了了。 </p>
</blockquote>
<blockquote>
<p>ReLU：整合线性单元,现在很多深度学习的模型，神经元一般都用ReLU的形式，其偏导数要么就是0要么就是1，乘起来就避免了梯度消失的问题，如果是1就不会改变前面的，如果是0就表示不更新该点了。<br>X&lt;0,就&#x3D;0<br>X&gt;0,就&#x3D;x  </p>
</blockquote>
<h2 id="第六章-支持向量机"><a href="#第六章-支持向量机" class="headerlink" title="第六章 支持向量机"></a>第六章 支持向量机</h2><pre><code>SVM能解决线性可分问题，怎么解决线性不可分问题（1.高维，2.核化）
SVM允许值可错误分类(软间隔)
SVM思想，推导过程，怎么得到优化问题？不用推理求解。
核方法：核函数的使用条件和使用对象。
</code></pre>
<ul>
<li><strong>基本思想</strong>：我们希望能找到一条线或者超平面把正类和反类划分开来，但是这样的线很多，我们该挑哪一个模型最优，哪一条泛化能力最强，可以从图上知道，正中间的线是泛化能力最强的。这就是支持向量机要解决的问题。<img src="image\支持向量机示意图.png">  
最影响线的选择的主要依靠最里面的这些样本点，我们把这些样本点称为**支持向量**.
<img src="image\支持向量.png"></li>
</ul>
<p>如果偏了的话，无论往哪边偏，正中间的间隔才是最大的，只要偏了，间隔就会变小。所以<strong>优化目标</strong>就是<strong>最大间隔</strong>。最大间隔 $\gamma&#x3D;\frac{2}{||w||}$,这个式子是通过$r&#x3D;\frac{w^Tx+b}{||w||}$，计算得到的，$\gamma&#x3D;2\cdot r$,分类为正的直线方程为$w^Tx+b&#x3D;1$，分类为负的直线方程为$w^Tx+b&#x3D;-1$</p>
<p><strong>分类正确的条件（约束条件）</strong>:  $y_i(w^Tx+b)\ge 1$<br>因为$w^Tx+b\ge 1$时，$y_i&#x3D;1$, 所以 $y_i(w^Tx+b)\ge 1$<br>$w^Tx+b\le -1$时，$y_i&#x3D;-1$, 所以 $y_i(w^Tx+b)\ge 1$<br>这两种情况都是分类正确的。</p>
<ul>
<li><strong>解的稀疏性</strong>：训练完成后，最终模型仅与支持向量有关。</li>
</ul>
<p><strong>以上是支持向量机SVM的基本型</strong>———————</p>
<ul>
<li><p><strong>支持向量机的缺点一</strong>：可能只能求解线性可分问题，异或问题或非线性可分就没办法。<br><strong>解决方法：</strong> 特征空间映射，将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间线性可分。且如果原始空间是有限维，即属性有限，那么一定存在这样的超平面。<br>但是这时候又遇到了高维特征空间计算内积的困难，高维空间里内积的计算就比较麻烦了，如果空间的维数相对比较低还好，非常高就很难计算了，<strong>主要的思路就是用核函数代替内积的计算。</strong> <strong>避免了高维空间的内积计算和绕过显式的特征映射。</strong><br><strong>核函数</strong>：Mercer定理：若一个**对称函数($\kappa (x_i,x_j)&#x3D;\kappa (x_j,x_i)$)<strong>所对应的</strong>核矩阵半正定$(\kappa &#x3D; \left[ {\begin{array}{ccccc}<br>  \kappa (x_1,x_1) &amp; \cdots &amp;\kappa (x_1,x_j) &amp;\cdots &amp;\kappa (x_1,x_m) \<br>  \vdots &amp; \ddots &amp;\vdots &amp;\ddots&amp;\vdots \<br>  \kappa (x_i,x_1) &amp; \cdots &amp;\kappa (x_i,x_j) &amp;\cdots &amp;\kappa (x_i,x_m) \<br>  \vdots &amp; \ddots &amp;\vdots &amp;\ddots&amp;\vdots \<br>  \kappa (x_m,x_1) &amp; \cdots &amp;\kappa (x_m,x_j) &amp;\cdots &amp;\kappa (x_m,x_m) \<br>\end{array} } \right])$**，那么它就可以作为核函数来使用</p>
<blockquote>
<p>SMO算法：同时优化两个参数<br>核函数的选择对支持向量机的性能很关键<br>基于经验，文本数据常常采用线性核，情况不明时常用高斯核。<br>核方法不仅仅可以用在支持向量机上。<br>以上就是核化支持向量机</p>
</blockquote>
</li>
<li><p><strong>支持向量机的缺点二</strong>：要每一个样本满足分类正确的条件，这个条件太严苛了。现实中很难确定适合的核函数，使得训练样本在特征空间线性可分。也很难断定这个线性可分是不是由过拟合造成的。<br>于是引入软间隔的概念，可以允许有支持向量机在一些样本上出错。<br>在优化目标加入惩罚项，C是惩罚系数和正则化系数，一个常数&gt;0，C越大对样本的惩罚越大。如果分类正确就没有惩罚，如果分类不正确就会有惩罚。  </p>
<blockquote>
<p>一些替代损失函数,软间隔支持向量机（引入松弛变量）<br>预测正确的时候，松弛变量就是0。<br>预测错误的时候，松弛变量就是$1-y_i(w^Tx_i+b)$<br>每一个样本都有自己的松弛变量ξi，i&#x3D;1，2，3……<br>松弛变量主要是简化我们对放宽的描述<br><img src="image\软间隔支持向量机.png"><br>从减少过拟合的风险看，可称之为正则化。该正则化和神经网络的不太一样，这里正则项是描述模型本身的某些性质<br><img src="image\支持向量机正则化.png"></p>
</blockquote>
</li>
</ul>
<blockquote>
<p>支持向量回归：允许模型输出和实际结果有2$\varepsilon$的差别。可以认为离直线很近的点这部分点就是在直线上，不计算误差，远的就计算误差损失。<br>优化目标：间隔最大，且损失最少。</p>
</blockquote>
<h2 id="第七章-贝叶斯分类器"><a href="#第七章-贝叶斯分类器" class="headerlink" title="第七章 贝叶斯分类器"></a>第七章 贝叶斯分类器</h2><pre><code>贝叶斯分类器的基本思想；
朴素贝也是分类器（计算题）；
半朴素、贝叶斯网、EM等理解就好，不需要掌握推导。
</code></pre>
<p>总的损失就是<strong>条件风险</strong>，分类错误的概率乘以损失&#x3D;条件风险，分类正确则没有损失。我们希望，分类的时候风险尽可能越小越好。所以最理想的一个模型就是使得条件风险最小对应的一个c，因为ci是一个类别，哪个类别使得条件风险最小，这个类别就是最优的，所以用argmin。<strong>可以理解目标为最小分类错误率。</strong><br><img src="image\贝叶斯决策论.png"><br>优化目标函数推理过程：<br><img src="image\贝叶斯推理过程.PNG">  </p>
<ul>
<li><p>判别式模型：概率作为输出结果，把x输入到模型m会输出一个概率，判断其属于哪一类</p>
</li>
<li><p>生成式：对联合概率分布建模而不是后验概率</p>
<img src="image\生成式VS判别式.png">
</li>
<li><p>建模的时候一般把<strong>证据去掉</strong>。<strong>先验概率看样本比例</strong>，<strong>类条件概率是我们需要估计的</strong>，这里用到<strong>极大似然估计</strong>去确认里面的参数，极大似然估计很依赖假设的概率分布形式，但是这个分布形式很难确定，求出来也不现实，很难将所有属性上的联合概率从有限的样本上获得，组合爆炸，样本稀疏，很多样本取值在训练集中根本没出现，所以不能直接用频率来估计类条件概率。</p>
<img src="image\贝叶斯定理.png">
</li>
<li><p>朴素（NB）贝叶斯分类器</p>
<img src="image\朴素贝叶斯分类器.PNG">
因为对所有类别来说，P(x)都相同，所以比较分子就可以了。  </li>
<li><p>课本P151</p>
</li>
</ul>
<p>$P(好瓜&#x3D;是|测试集1)&#x3D;P(好瓜&#x3D;是)\Pi_{i&#x3D;1}^d P(x_i|好瓜&#x3D;是)$<br>$P(好瓜&#x3D;否|测试集1)&#x3D;P(好瓜&#x3D;否)\Pi_{i&#x3D;1}^d P(x_i|好瓜&#x3D;否)$<br>$P(x_i|好瓜&#x3D;是)&#x3D;\frac{D_{c,x_i}}{D_c} 或者 \frac{1}{\sqrt{2\pi}\delta_{c,i}}exp(-\frac{(x_i-\mu_{c,i})^2}{2\delta_{c,i}^2})$<br>离散的直接算，连续的要计算训练集属性均值和方差。  </p>
<ul>
<li><p>拉普拉斯修正：万一出现了训练集没出现过的属性值就会因为连乘而影响计算结果，所以在计算P(c)时分子加1，分母加2（好瓜坏瓜两种类别），P（xi|c)时，分子加1，分母加该3（该属性的可能取值：青绿、乌黑、浅白）</p>
</li>
<li><p>朴素贝叶斯的应用：1.分辨垃圾邮件，2.任务对预测速度要求高，3.任务数据更替频繁，4.数据不断增加</p>
</li>
</ul>
<blockquote>
<p>半朴素贝叶斯：所有属性都条件独立是很难的，于是有了独依赖，父属性（所有属性依赖于同一个属性：超父）<br>贝叶斯网；道德图（把贝叶斯网的方向删掉，并把父结点连起来）<br>EM算法对不完整训练集的模型参数的估计。</p>
</blockquote>
<h2 id="第八章-集成学习"><a href="#第八章-集成学习" class="headerlink" title="第八章 集成学习"></a>第八章 集成学习</h2><pre><code>集成学习的基本概念；
什么是好的集成？什么条件？
3个对应的例子：  
序列式、并行式（代表算法）
结合策略的好处，有哪些策略
多样性：主要掌握怎么提升多样性并举例，误差与分歧不用掌握
</code></pre>
<p><strong>集成学习</strong>：通过构建并结合多个学习器来完成学习任务，为获得比单一学习器更优越的泛化性能。<br><strong>个体学习器</strong>：由一个算法从训练数据中产生。<br><strong>同质集成</strong>：同质集成中的<strong>个体学习器</strong>都是同种类型的，也称“<strong>基学习器</strong>”，相应的算法叫<strong>基学习算法</strong><br><strong>异质集成</strong>：异质集成中的<strong>个体学习器</strong>可包含不同类型的，也称“<strong>组件学习器</strong>”。  </p>
<ul>
<li><p>集成学习是针对弱学习器进行的，但要注意弱学习器至少也要略优于随机猜测的学习器。  </p>
</li>
<li><p>集成个体应好而不同：（P172)<br>即个体学习器要有一定的“准确性”，不能太坏，并且要有“多样性”，即学习器之间具有差异。（事实上，准确性和多样性存在冲突，所以这是集成学习研究的核心）<br>随着集成中个体分类器数目T的增大，集成的错误率指数级下降。  </p>
</li>
<li><p>集成学习方法两大类：序列化方法（串行生成）<strong>Boosting</strong>，并行化方法 <strong>Bagging</strong> 和 <strong>随机森林</strong> 。 </p>
</li>
<li><p><strong>Boosting</strong>:先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整（保证了多样性），<strong>使得先前的基学习器做错的训练样本在后续受到更多关注</strong>。（修改权重）  ；<strong>重采样法</strong>可以避免训练过早停止（因为如果当前轮次基学习器不如随机猜测的话，学习就会停止，重采样法给了重启动的机会）Boosting关注<strong>降低偏差</strong>，所以能基于相当弱的学习器构建出很强的集成。</p>
</li>
<li><p>Boosting族著名代表：<strong>AdaBoost</strong>：最后集成的决定由各学习器的加权投票决定（<strong>正确率高的学习器让它的权重更大一些</strong>），目标是最小化指数损失函数。  </p>
</li>
<li><p><strong>Bagging</strong> 和 <strong>随机森林</strong>：想要得到泛化性能强的集成，并行式面对的问题要增加多样性，所以训练集的确定方式很重要，要设法使基学习器尽可能具有较大的差异，但学习的性能又不会太差，不能都用不同的训练集，这就要考虑使用有互相交叠的采集子样。</p>
</li>
<li><p><strong>Bagging</strong>：<strong>自助采样法</strong>，“外包估计”还可以辅助剪枝或者早停。与AdaBoost只适用于二分类不同，这个还可以用于多分类和回归，预测结果分类采用投票法、回归采用简单平均法输出。主要关注降低方差，减少易受样本扰动的学习器上的影响，如<em>不剪枝的决策树和神经网络</em></p>
</li>
<li><p><strong>随机森林</strong>：RF在以决策树为基学习器构建的Bagging集成的基础上，进一步再决策树的训练过程中引入了<strong>随机属性</strong>的选择。（进一步增加量多样性），随机森林的训练效率常优于Bagging，因为Bagging考虑所有属性，而随机森林只考虑一个属性子集。</p>
</li>
<li><p><strong>结合策略</strong>：<strong>学习器结合的三个好处</strong>：1.从统计上，学习任务的假设空间往往很大，可能在多个假设下训练的性能相等，单学习器可能因为误选导致泛化性能不佳，没选到性能更好的假设，结合多个学习器可以减少这一风险。2.从计算上，学习算法往往会陷入局部极小，导致泛化性能糟糕，结合多个学习器可以降低该风险。3.从表示方面来看，学习任务的假设空间可能都没被当前学习算法考虑，这样的单学习器是无效的，所以结合多学习器能扩大假设空间，可能学得更好的近似。<br><strong>策略</strong>：<strong>平均法、投票法、学习法</strong>：通过与另一个学习器来结合，Stacking，个体学习器是初级学习器，用于结合的学习器是次级学习器或者元学习器。用初级学习器的输出作为样例的输入特征，初始样本的标记仍被当作样例标记，训练次级学习器。</p>
</li>
</ul>
<blockquote>
<p>误差-分歧分解：$E&#x3D;\bar E-\bar A$，$\bar E$是个体学习器泛化误差的加权均值，$\bar A$是个体学习器的加权分歧，分歧越大，学习器泛化误差越小，集成的泛化误差越小。</p>
</blockquote>
<ul>
<li>多样性增强：<strong>1. 数据样本扰动（采样法）</strong>，有些机器学习方法对样本扰动不敏感如线性、支持向量、朴素贝叶斯、k近邻等叫稳定基学习器，需要考虑其他方法。；<strong>2. 输入属性扰动</strong>（抽取若干属性子集，分别训练学习器）；<strong>3. 输出表示扰动</strong>（ECOC法：把多分类问题转换为一系列二分类问题、翻转法：把分类输出转换为回归）；<strong>4. 算法参数扰动</strong>， 随机设置一些超参数，产生不同的个体学习器。</li>
</ul>
<blockquote>
<p>选择性集成：对并行化集成修剪，即在集成产生后试图去除一些个体学习器来获得较小的集成，能在减小规模同时提高泛化性能。</p>
</blockquote>
<h2 id="第九章-聚类"><a href="#第九章-聚类" class="headerlink" title="第九章 聚类"></a>第九章 聚类</h2><pre><code>（无监督）三种聚类方法考一种，理解思想和构建流程，连续、离散、混合
</code></pre>
<ul>
<li><p>聚类的主要思想：将数据集中的样本划分为若干个通常是不相交的子集。   </p>
</li>
<li><p>$\lambda_j$:簇标记，标记该样本属于哪一簇</p>
</li>
<li><p><strong>性能度量</strong>：聚类的“有效性指标”：“外部指标（有参考模型）”，“内部指标（无参考模型，直接观察聚类结果）”</p>
</li>
<li><p><strong>距离计算</strong>：非负性；同一性；对称性；直递性。<br><strong>连续属性</strong>可用欧式距离（闵可夫斯基距离p&#x3D;2），曼哈顿距离（闵可夫斯基距离p&#x3D;1），<strong>离散属性</strong>可使用VDM距离，即把不同簇中取两个属性值的样本数占总体两个属性值样本数的比例相减的p次方求和。<strong>混合属性</strong>可以把闵可夫斯基距离和VDM结合。</p>
</li>
<li><p>我们是在<strong>通过某种形式的距离来定义“相似度度量”</strong>，距离越大相似度越小，不满足直递性的叫非度量距离。</p>
</li>
<li><p><strong>原型</strong>：反映数据特征</p>
</li>
<li><p><strong>k均值算法</strong>思想：聚类所得簇的最小化平方误差，想让簇内样本相似度越高，簇内样本围绕簇均值向量的紧密程度。</p>
<ul>
<li>步骤：（贪心策略）1. 从样本集D中随机选k个样本作为初始均值向量{$\mu_1,\dots,\mu_k$} 2. 计算各样本$x_j$与各向量均值的距离$d_i$ 3. 根据距离最近的均值向量确定$x_j$的簇标记 4.将样本$x_j$划入相应的簇 5. 计算新的均值向量：$\mu_i’$并更新。循环直到均值向量均未更新。</li>
<li>学习向量量化：是有监督学习，输出的是原型向量，试图找到一组原型向量来刻画聚类结构，</li>
<li>高斯混合聚类：是采用<strong>概率</strong>模型(高斯分布)对原型进行刻画。哪个样本被分如哪个簇的概率最大。</li>
</ul>
</li>
<li><p><strong>密度聚类</strong>思想：假设聚类结构能够通过样本分布的紧密程度确定。<strong>DBSCAN</strong>:  核心对象（领域内至少有MinPts（&#x3D;3）个样本）；密度直达；密度可达；密度相连.</p>
<ul>
<li>DBSCAN将簇定义为：由密度可达关系导出的最大的密度相连的样本集合。</li>
<li>步骤：1. 先找出各样本的$\epsilon$ -领域并确定核心对象集合，任选一个核心对象作为“种子” 2. 找出由其密度可达的所有样本，这就构成了一个聚类簇$C_1$。3. 将$C_1$中包含的核心对象从核心对象集中去除 4. 再从更新后的核心对象集抽一个作为种子，循环上述步骤 5. 知道核心对象集为空。</li>
</ul>
</li>
<li><p><strong>层次聚类</strong>思想：试图在<strong>不同层次</strong>对数据集进行划分，从而形成树形的聚类结构。可自底向上也可自顶向下。</p>
<ul>
<li><strong>（AGNES）</strong>是一种采用自底向上策略的层次聚类法，<strong>思想</strong>：将数据集中每一个样本都看作是一个初始聚类簇，然后找出距离最近的两个聚类簇合并，该过程不断重复，直到达到预设的聚类簇数。</li>
<li>距离定义：最大距离（簇内最远样本）、最小距离（簇内最近样本）、平均距离（簇内所有样本）</li>
<li>步骤：1. 针对样本的初始聚类簇，计算相应的初始化距离矩阵。2. 循环合并距离最近的聚类簇，合并后更新相应的距离矩阵。 3.直到达到预设的聚类簇数。</li>
</ul>
</li>
</ul>
<h2 id="第十章-降维与度量学习"><a href="#第十章-降维与度量学习" class="headerlink" title="第十章 降维与度量学习"></a>第十章 降维与度量学习</h2><pre><code>降维理论的方法和思想；
k近邻学习的思想；
非线性掌握一下原理即可
</code></pre>
<ul>
<li><p><strong>k近邻学习（KNN）</strong>：监督学习方法，思想：给定测试样本，基于某种距离度量找出与其最靠近的k个训练样本，根据这些训练样本的结果信息对测试样本进行预测。常用的分类有“投票法”：将这k个样本中出现最多的类别标记为预测结果，回归有“平均法”，将k个样本的值平均作为预测结果，还可以根据距离远近加权。</p>
<ul>
<li>这是一种懒惰学习，训练时间开销小，但k值的选取很重要，距离度量的方法也是影响算法好坏的因素。其泛化错误率不超过贝叶斯最优分类器的两倍。</li>
</ul>
</li>
<li><p><strong>密采样</strong>：样本采样的密度足够大；<strong>维数灾难</strong>：在高维空间内出现对距离的计算、样本稀疏等问题。</p>
<ul>
<li>缓解维数灾难的措施 <strong>“降维”</strong>（线性、非线性） ，为什么可以降维呢？因为有时候人们收集到的数据是高维的，但学习任务可能只与低维的分布有关，即高维空间中的一个<strong>低维嵌入</strong>。</li>
</ul>
</li>
<li><p><strong>多维缩放(MDS)</strong>:为了保留距离的特征，目的是为了在低维空间里样本的欧氏距离（约）等于原始空间的距离。对原始高维空间进行<strong>线性</strong>变换获得低维子空间。“对降维效果的评估”：根据降维前后学习器的性能; 降到二到三维可以可视化看到。</p>
</li>
<li><p><strong>主成分分析（PCA）</strong>：超平面作为样本投影平面的两个原则：最近重构性，样本到超平面的距离足够近；最大可分性，样本在这个超平面上的投影要尽可能分开（<strong>投影后方差最大</strong>，为主成分分析的优化目标）。</p>
<ul>
<li>但是低维的维数$d’$通常要自己人为指定，所以可以用不同$d’$进行k近邻（或者其他开销小的学习器）交叉验证，也可以选择阈值，降维后的特征值之后占原特征值之和的比例&gt;95%之类的。</li>
<li>对于被舍弃的信息（特征值低的特征向量），一、降维增加了样本采样密度。二、在一定程度上能去噪，因为往往小的特征值对应的特征向量与噪声相关。</li>
</ul>
</li>
<li><p><strong>核化线性降维</strong>：在不少现实任务中需要非线性映射才能找到恰当的低维嵌入。基于核技巧的线性降维，<strong>核主成分分析</strong>，把样本映射到高维特征空间再在特征空间中实施PCA.</p>
</li>
<li><p><strong>流形学习</strong>，等度量映射，认为低维流行嵌入到高维空间后，直接在高维空间计算直线距离是具有误导性的。（<strong>测地线距离</strong>）</p>
</li>
<li><p><strong>局部线性嵌入</strong>：保持关系</p>
</li>
<li><p><strong>度量学习</strong>：对高维数据的降维主要目的是为了找到一个合适的低维空间，寻找合适的空间本质上就是找一个合适的距离度量。希望能“学习”出一个合适的距离度量。</p>
</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/08/15/hello-world/" rel="prev" title="Hello World">
      <i class="fa fa-chevron-left"></i> Hello World
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86"><span class="nav-number">1.</span> <span class="nav-text">知识点整理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E5%BD%95"><span class="nav-number">1.0.1.</span> <span class="nav-text">目录</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BB%AA%E8%AE%BA"><span class="nav-number">1.1.</span> <span class="nav-text">第一章 绪论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E5%92%8C%E9%80%89%E6%8B%A9"><span class="nav-number">1.2.</span> <span class="nav-text">第二章 模型的评估和选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.</span> <span class="nav-text">第三章 线性模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">1.4.</span> <span class="nav-text">第四章 决策树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.5.</span> <span class="nav-text">第五章 神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-number">1.6.</span> <span class="nav-text">第六章 支持向量机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">1.7.</span> <span class="nav-text">第七章 贝叶斯分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.8.</span> <span class="nav-text">第八章 集成学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B9%9D%E7%AB%A0-%E8%81%9A%E7%B1%BB"><span class="nav-number">1.9.</span> <span class="nav-text">第九章 聚类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E7%AB%A0-%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.10.</span> <span class="nav-text">第十章 降维与度量学习</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Author</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Author</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
